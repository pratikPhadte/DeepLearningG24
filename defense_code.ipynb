{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253c107-980e-4810-b617-86e9a75558c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch transformers datasets textattack textblob matplotlib scikit-learn\n",
    "from datasets import load_dataset\n",
    "imdb_reviews = load_dataset(\"imdb\")\n",
    "train_reviews, train_labels = imdb_reviews['train']['text'], imdb_reviews['train']['label']\n",
    "test_reviews, test_labels = imdb_reviews['test']['text'], imdb_reviews['test']['label']\n",
    "print(f\"Train reviews from IMDB: {len(train_reviews)}\")\n",
    "print(f\"Test reviews from IMDB: {len(test_reviews)}\")\n",
    "from transformers import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = GPU, -1 = CPU\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=bert_tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc84d0da-f35d-4d03-b713-e4b671240139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform batched inference\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 32\n",
    "tokenized_data = preprocess_texts(test_reviews, bert_tokenizer)\n",
    "\n",
    "# Create DataLoader for batched processing\n",
    "test_dataset = TensorDataset(\n",
    "    tokenized_data['input_ids'], tokenized_data['attention_mask']\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask = [b.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "# Convert predictions to labels\n",
    "all_preds_labels = [\"POSITIVE\" if pred == 1 else \"NEGATIVE\" for pred in all_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15476a-69d9-4910-9dc2-148badda7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "clean_accuracy = accuracy_score(test_labels, all_preds)\n",
    "print(f\"Clean data accuracy: {clean_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5395c5-1bb2-43f3-b002-bed0e89cf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87060e5a-6656-439d-a8f7-8d2b6e225cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autocorrect in /home/jovyan/.local/lib/python3.8/site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:27:17.908565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-31 11:27:18.051285: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-31 11:27:20.010865: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-31 11:27:20.010981: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-31 11:27:20.010987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29f498f-77ce-4ce2-a961-44f4a1507ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews from IMDB: 1500\n",
      "Test reviews from IMDB: 1500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Function to load data\n",
    "def load_data(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        dir_path = os.path.join(directory, label_type)\n",
    "        for file in os.listdir(dir_path):\n",
    "            with open(os.path.join(dir_path, file), 'r', encoding='utf-8') as f:\n",
    "                data.append(f.read())\n",
    "            labels.append(1 if label_type == 'pos' else 0)  # Positive: 1, Negative: 0\n",
    "    return data, labels\n",
    "\n",
    "# Load training and test data\n",
    "train_dir = '/home/jovyan/DL/aclImdb/train'\n",
    "test_dir = '/home/jovyan/DL/aclImdb/test'\n",
    "train_reviews, train_labels = load_data(train_dir)\n",
    "test_reviews, test_labels = load_data(test_dir)\n",
    "\n",
    "# Slice the data to use only 1500 samples\n",
    "train_reviews = train_reviews[:1500]\n",
    "train_labels = train_labels[:1500]\n",
    "test_reviews = test_reviews[:1500]\n",
    "test_labels = test_labels[:1500]\n",
    "print(f\"Train reviews from IMDB: {len(train_reviews)}\")\n",
    "print(f\"Test reviews from IMDB: {len(test_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed187398-31fc-45ad-bfa4-0f9abec6b048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and sanitize text data.\n",
    "    - Converts to lowercase\n",
    "    - Removes URLs, punctuation, and extra spaces\n",
    "    - Corrects spelling errors\n",
    "    \"\"\"\n",
    "    spell = Speller(lang='en')\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = spell(text)  # Correct spelling\n",
    "    return text\n",
    "\n",
    "def preprocess_data(texts):\n",
    "    \"\"\"\n",
    "    Apply text cleaning to the dataset.\n",
    "    \"\"\"\n",
    "    return [clean_text(text) for text in tqdm(texts, desc=\"Cleaning Data\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd42f5a-e586-4ba9-a86c-03de80415d41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Data: 100%|██████████| 1500/1500 [18:43<00:00,  1.34it/s]\n",
      "Cleaning Data: 100%|██████████| 1500/1500 [19:17<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Preprocess the data\n",
    "train_reviews = preprocess_data(train_reviews)\n",
    "test_reviews = preprocess_data(test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c7cd3e8-5f80-483c-a8c1-d3c8428fbd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_pgd_adversarial_examples(model, tokenizer, text, label, epsilon=0.1, alpha=0.02, num_iter=5):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples using PGD on embeddings.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "    # Extract embeddings\n",
    "    embeddings = model.bert.embeddings.word_embeddings(inputs[\"input_ids\"]).detach().clone()\n",
    "    perturbed_embeddings = embeddings.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        # Forward pass with perturbed embeddings\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        outputs = model(inputs_embeds=perturbed_embeddings, attention_mask=attention_mask, labels=label_tensor)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient step\n",
    "        grad_sign = perturbed_embeddings.grad.sign()\n",
    "        perturbed_embeddings = perturbed_embeddings + alpha * grad_sign\n",
    "\n",
    "        # Project back to the epsilon-ball\n",
    "        perturbation = torch.clamp(perturbed_embeddings - embeddings, -epsilon, epsilon)\n",
    "        perturbed_embeddings = torch.clamp(embeddings + perturbation, -1, 1).detach().requires_grad_(True)\n",
    "\n",
    "    # Convert perturbed embeddings back to tokens\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs_embeds=perturbed_embeddings, attention_mask=attention_mask).logits\n",
    "        perturbed_input_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    perturbed_text = tokenizer.decode(perturbed_input_ids[0], skip_special_tokens=True)\n",
    "    return perturbed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90abfd0-f370-42c6-b498-17d42a9a57ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adversarial_training_with_augmentation(model, tokenizer, train_texts, train_labels, num_epochs=3, epsilon=0.1, alpha=0.02, num_iter=5):\n",
    "    \"\"\"\n",
    "    Train a model with adversarial training.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for text, label in tqdm(zip(train_texts, train_labels), total=len(train_texts), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Generate adversarial examples\n",
    "            adversarial_text = generate_pgd_adversarial_examples(model, tokenizer, text, label, epsilon, alpha, num_iter)\n",
    "\n",
    "            # Tokenize original and adversarial text\n",
    "            inputs_original = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            inputs_adversarial = tokenizer(adversarial_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            label_tensor = torch.tensor([label]).to(device)\n",
    "\n",
    "            # Forward pass with original and adversarial inputs\n",
    "            outputs_original = model(**inputs_original)\n",
    "            outputs_adversarial = model(**inputs_adversarial)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_original = loss_fn(outputs_original.logits, label_tensor)\n",
    "            loss_adversarial = loss_fn(outputs_adversarial.logits, label_tensor)\n",
    "\n",
    "            loss = (loss_original + loss_adversarial) / 2  # Combine losses\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_texts):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efdfc3f-7f31-461a-96e2-c8bf01596131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_defense(model, tokenizer, test_texts, test_labels, epsilon=0.1, alpha=0.02, num_iter=5):\n",
    "    \"\"\"\n",
    "    Evaluate the defense against adversarial examples.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    clean_preds = []\n",
    "    adv_preds = []\n",
    "    attack_success_count = 0\n",
    "\n",
    "    for text, label in tqdm(zip(test_texts, test_labels), total=len(test_texts), desc=\"Evaluating Defense\"):\n",
    "        # Clean prediction\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        logits = model(**inputs).logits\n",
    "        clean_pred = torch.argmax(logits, dim=-1).item()\n",
    "        clean_preds.append(clean_pred)\n",
    "\n",
    "        # Generate adversarial example\n",
    "        adv_text = generate_pgd_adversarial_examples(model, tokenizer, text, label, epsilon, alpha, num_iter)\n",
    "\n",
    "        # Adversarial prediction\n",
    "        adv_inputs = tokenizer(adv_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        adv_logits = model(**adv_inputs).logits\n",
    "        adv_pred = torch.argmax(adv_logits, dim=-1).item()\n",
    "        adv_preds.append(adv_pred)\n",
    "\n",
    "        # Check if the attack was successful\n",
    "        if clean_pred == label and adv_pred != label:\n",
    "            attack_success_count += 1\n",
    "\n",
    "    # Compute Metrics\n",
    "    clean_accuracy = accuracy_score(test_labels, clean_preds) * 100\n",
    "    adversarial_accuracy = accuracy_score(test_labels, adv_preds) * 100\n",
    "    performance_drop = clean_accuracy - adversarial_accuracy\n",
    "    attack_success_rate = (attack_success_count / len(test_texts)) * 100\n",
    "\n",
    "    return clean_accuracy, adversarial_accuracy, performance_drop, attack_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1b1fe3-a17e-4b2f-bdc5-fe3d370513c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0175201066704ef2975a5b2371f8fa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06391175fd954c5b93ccf2bb36c932ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fed6ad9368432385d8c54ef06d81ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ef0fafc57b40f88383dd9e628b7e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed456bbc354d44c08ee023608f940c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"textattack/bert-base-uncased-SST-2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9acc12a2-f3a6-43b9-8958-7773a849aa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aafbf0eb9b43dfb0e15dd6bf78e2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 1500/1500 [07:22<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Defense: 100%|██████████| 1500/1500 [07:59<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defense Evaluation Metrics ---\n",
      "Clean Accuracy: 100.00%\n",
      "Adversarial Accuracy: 100.00%\n",
      "Performance Drop: 0.00%\n",
      "Attack Success Rate: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Adversarial Training\n",
    "adversarial_training_with_augmentation(model, tokenizer, train_reviews, train_labels, num_epochs=1, epsilon=0.1, alpha=0.02, num_iter=3)\n",
    "\n",
    "# Evaluate Defense\n",
    "clean_acc, adv_acc, perf_drop, attack_success_rate = evaluate_defense(\n",
    "    model, tokenizer, test_reviews, test_labels, epsilon=0.1, alpha=0.02, num_iter=5\n",
    ")\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n--- Defense Evaluation Metrics ---\")\n",
    "print(f\"Clean Accuracy: {clean_acc:.2f}%\")\n",
    "print(f\"Adversarial Accuracy: {adv_acc:.2f}%\")\n",
    "print(f\"Performance Drop: {perf_drop:.2f}%\")\n",
    "print(f\"Attack Success Rate: {attack_success_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615b23f-fd51-4bb3-9018-844506487d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
